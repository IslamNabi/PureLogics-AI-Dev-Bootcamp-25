{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_SnZhpdv2Jg"
   },
   "source": [
    "# Reading\n",
    "- https://www.unite.ai/nlp-rise-with-transformer-models-a-comprehensive-analysis-of-t5-bert-and-gpt/\n",
    "\n",
    "- https://aliissa99.medium.com/transformer-gpt-3-gpt-j-t5-and-bert-4cf8915dd86f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNByPy_Zxv_B"
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KztxiLcsw4lg"
   },
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from transformers import (\n",
    "    GPT2Model, GPT2Config,\n",
    "    BertModel, BertConfig,\n",
    "    T5Model, T5Config,\n",
    "    GPT2Tokenizer, BertTokenizer, T5Tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNXG1wR_w4jC"
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJuUfMlDw4gS"
   },
   "outputs": [],
   "source": [
    "# Part 1: GPT Architecture Implementation (Decoder-only)\n",
    "class GPTSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.query = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.key = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.value = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "\n",
    "        # Project query, key, value\n",
    "        q = self.query(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn_weights = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        # Apply causal mask (lower triangular)\n",
    "        causal_mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool().to(device)\n",
    "        attn_weights = attn_weights.masked_fill(causal_mask, float('-inf'))\n",
    "\n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            attn_weights = attn_weights.masked_fill(attention_mask.unsqueeze(1).unsqueeze(2), float('-inf'))\n",
    "\n",
    "        # Normalize and apply attention\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "\n",
    "        # Reshape and project output\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, self.embed_dim)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PPta6HEw4ds"
   },
   "outputs": [],
   "source": [
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attn = GPTSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.hidden_size)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.intermediate_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output = self.attn(self.ln_1(x), attention_mask)\n",
    "        x = x + attn_output\n",
    "\n",
    "        # MLP with residual connection\n",
    "        mlp_output = self.mlp(self.ln_2(x))\n",
    "        x = x + mlp_output\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdcCipqQxBGI"
   },
   "outputs": [],
   "source": [
    "class SimpleGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n",
    "        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "\n",
    "        self.blocks = nn.ModuleList([GPTBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.ln_f = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        device = input_ids.device\n",
    "        batch_size, seq_length = input_ids.size()\n",
    "\n",
    "        position_ids = torch.arange(0, seq_length, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        # Get embeddings\n",
    "        inputs_embeds = self.wte(input_ids)\n",
    "        position_embeds = self.wpe(position_ids)\n",
    "        hidden_states = inputs_embeds + position_embeds\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "\n",
    "        # Process through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            hidden_states = block(hidden_states, attention_mask)\n",
    "\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCWHHXotxBDh"
   },
   "outputs": [],
   "source": [
    "# Part 2: BERT Architecture Implementation (Encoder-only)\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = config.hidden_size // config.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        # Apply the attention mask (if provided)\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        return context_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeMiXFQwxBAw"
   },
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = BertSelfAttention(config)\n",
    "        self.attention_output = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.attention_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.attention_ln = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "        self.intermediate = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.output = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.output_ln = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        attention_output = self.attention_output(attention_output)\n",
    "        attention_output = self.attention_dropout(attention_output)\n",
    "        attention_output = self.attention_ln(attention_output + hidden_states)\n",
    "\n",
    "        intermediate_output = F.gelu(self.intermediate(attention_output))\n",
    "        layer_output = self.output(intermediate_output)\n",
    "        layer_output = self.output_dropout(layer_output)\n",
    "        layer_output = self.output_ln(layer_output + attention_output)\n",
    "\n",
    "        return layer_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdJ26YwdxIeS"
   },
   "outputs": [],
   "source": [
    "class SimpleBERT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embeddings_token = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.embeddings_position = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.embeddings_token_type = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.embeddings_ln = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.embeddings_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        self.encoder = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        # Embed tokens, positions, and token types\n",
    "        token_embeddings = self.embeddings_token(input_ids)\n",
    "        position_embeddings = self.embeddings_position(position_ids)\n",
    "        token_type_embeddings = self.embeddings_token_type(token_type_ids)\n",
    "\n",
    "        # Sum all embeddings\n",
    "        embeddings = token_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.embeddings_ln(embeddings)\n",
    "        embeddings = self.embeddings_dropout(embeddings)\n",
    "\n",
    "        # Prepare attention mask\n",
    "        if attention_mask is not None:\n",
    "            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        else:\n",
    "            extended_attention_mask = None\n",
    "\n",
    "        # Process through the encoder layers\n",
    "        hidden_states = embeddings\n",
    "        for layer in self.encoder:\n",
    "            hidden_states = layer(hidden_states, extended_attention_mask)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uug3h-qexIb6"
   },
   "outputs": [],
   "source": [
    "# Part 3: T5 Architecture Implementation (Encoder-Decoder)\n",
    "class T5SelfAttention(nn.Module):\n",
    "    def __init__(self, config, is_decoder=False):\n",
    "        super().__init__()\n",
    "        self.is_decoder = is_decoder\n",
    "        self.d_model = config.d_model\n",
    "        self.n_heads = config.num_heads\n",
    "        self.d_kv = config.d_kv\n",
    "\n",
    "        self.q = nn.Linear(self.d_model, self.n_heads * self.d_kv)\n",
    "        self.k = nn.Linear(self.d_model, self.n_heads * self.d_kv)\n",
    "        self.v = nn.Linear(self.d_model, self.n_heads * self.d_kv)\n",
    "        self.o = nn.Linear(self.n_heads * self.d_kv, self.d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "\n",
    "    def forward(self, hidden_states, mask=None, kv=None, position_bias=None):\n",
    "        batch_size, seq_length, _ = hidden_states.shape\n",
    "\n",
    "        q = self.q(hidden_states).view(batch_size, seq_length, self.n_heads, self.d_kv).transpose(1, 2)\n",
    "\n",
    "        if kv is not None:\n",
    "            _, kv_length, _ = kv.shape\n",
    "            k = self.k(kv).view(batch_size, kv_length, self.n_heads, self.d_kv).transpose(1, 2)\n",
    "            v = self.v(kv).view(batch_size, kv_length, self.n_heads, self.d_kv).transpose(1, 2)\n",
    "        else:\n",
    "            k = self.k(hidden_states).view(batch_size, seq_length, self.n_heads, self.d_kv).transpose(1, 2)\n",
    "            v = self.v(hidden_states).view(batch_size, seq_length, self.n_heads, self.d_kv).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.d_kv)\n",
    "\n",
    "        if position_bias is not None:\n",
    "            scores += position_bias\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Add causal mask for decoder\n",
    "        if self.is_decoder and kv is None:\n",
    "            causal_mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool().to(device)\n",
    "            scores = scores.masked_fill(causal_mask.unsqueeze(0).unsqueeze(0), -1e9)\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, self.n_heads * self.d_kv)\n",
    "        attn_output = self.o(attn_output)\n",
    "\n",
    "        return attn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhGeaPKKxSig"
   },
   "outputs": [],
   "source": [
    "class T5Block(nn.Module):\n",
    "    def __init__(self, config, is_decoder=False):\n",
    "        super().__init__()\n",
    "        self.is_decoder = is_decoder\n",
    "        self.layer_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n",
    "\n",
    "        self.self_attention = T5SelfAttention(config, is_decoder=is_decoder)\n",
    "\n",
    "        if is_decoder:\n",
    "            self.cross_attention = T5SelfAttention(config, is_decoder=False)\n",
    "            self.cross_layer_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.d_model, config.d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout_rate),\n",
    "            nn.Linear(config.d_ff, config.d_model),\n",
    "            nn.Dropout(config.dropout_rate)\n",
    "        )\n",
    "        self.mlp_layer_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None):\n",
    "        # Self attention\n",
    "        norm_x = self.layer_norm(hidden_states)\n",
    "        attention_output = self.self_attention(norm_x, mask=attention_mask)\n",
    "        hidden_states = hidden_states + attention_output\n",
    "\n",
    "        # Cross attention for decoder\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            norm_x = self.cross_layer_norm(hidden_states)\n",
    "            cross_attention_output = self.cross_attention(norm_x, mask=encoder_attention_mask, kv=encoder_hidden_states)\n",
    "            hidden_states = hidden_states + cross_attention_output\n",
    "\n",
    "        # Feed-forward network\n",
    "        norm_x = self.mlp_layer_norm(hidden_states)\n",
    "        ff_output = self.mlp(norm_x)\n",
    "        hidden_states = hidden_states + ff_output\n",
    "\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMOqN_3kxSf4"
   },
   "outputs": [],
   "source": [
    "class SimpleT5(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.ModuleList([T5Block(config) for _ in range(config.num_layers)])\n",
    "        self.encoder_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.ModuleList([T5Block(config, is_decoder=True) for _ in range(config.num_decoder_layers)])\n",
    "        self.decoder_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n",
    "\n",
    "    def forward(self, input_ids, decoder_input_ids=None, attention_mask=None, decoder_attention_mask=None):\n",
    "        # Embed inputs\n",
    "        inputs_embeds = self.shared(input_ids)\n",
    "\n",
    "        # Encoder\n",
    "        encoder_hidden_states = inputs_embeds\n",
    "        for layer in self.encoder:\n",
    "            encoder_hidden_states = layer(encoder_hidden_states, attention_mask)\n",
    "        encoder_hidden_states = self.encoder_norm(encoder_hidden_states)\n",
    "\n",
    "        # Return just encoder output if no decoder input\n",
    "        if decoder_input_ids is None:\n",
    "            return encoder_hidden_states\n",
    "\n",
    "        # Embed decoder inputs\n",
    "        decoder_inputs_embeds = self.shared(decoder_input_ids)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_hidden_states = decoder_inputs_embeds\n",
    "        for layer in self.decoder:\n",
    "            decoder_hidden_states = layer(\n",
    "                decoder_hidden_states,\n",
    "                decoder_attention_mask,\n",
    "                encoder_hidden_states,\n",
    "                attention_mask\n",
    "            )\n",
    "        decoder_hidden_states = self.decoder_norm(decoder_hidden_states)\n",
    "\n",
    "        return encoder_hidden_states, decoder_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWSViU5-xYXg"
   },
   "outputs": [],
   "source": [
    "# Part 4: Load and Compare Pre-trained Models\n",
    "def load_pretrained_models():\n",
    "    # Load GPT-2 model and tokenizer\n",
    "    gpt2_config = GPT2Config.from_pretrained('gpt2')\n",
    "    gpt2_model = GPT2Model.from_pretrained('gpt2')\n",
    "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "    # Load BERT model and tokenizer\n",
    "    bert_config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "    bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Load T5 model and tokenizer\n",
    "    t5_config = T5Config.from_pretrained('t5-small')\n",
    "    t5_model = T5Model.from_pretrained('t5-small')\n",
    "    t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "    return {\n",
    "        'gpt2': (gpt2_model, gpt2_tokenizer, gpt2_config),\n",
    "        'bert': (bert_model, bert_tokenizer, bert_config),\n",
    "        't5': (t5_model, t5_tokenizer, t5_config)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueOZNMtWxYUw"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def model_parameter_comparison():\n",
    "    models = load_pretrained_models()\n",
    "\n",
    "    param_counts = {}\n",
    "    for name, (model, _, _) in models.items():\n",
    "        param_counts[name] = count_parameters(model) / 1_000_000  # in millions\n",
    "\n",
    "    # Create a bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(param_counts.keys(), param_counts.values(), color=['blue', 'orange', 'green'])\n",
    "\n",
    "    # Add parameter count labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{height:.1f}M', ha='center', va='bottom')\n",
    "\n",
    "    plt.title('Parameter Count Comparison (in millions)')\n",
    "    plt.ylabel('Millions of Parameters')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return param_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "848HYo0Yxa2J"
   },
   "outputs": [],
   "source": [
    "# Part 5: Architecture Analysis\n",
    "def visualize_attention_patterns():\n",
    "    # Create sample inputs\n",
    "    text = \"The transformer architecture revolutionized natural language processing.\"\n",
    "\n",
    "    # Load models\n",
    "    models = load_pretrained_models()\n",
    "\n",
    "    _, gpt2_tokenizer, _ = models['gpt2']\n",
    "    _, bert_tokenizer, _ = models['bert']\n",
    "\n",
    "    # Tokenize input for each model\n",
    "    gpt2_tokens = gpt2_tokenizer.tokenize(text)\n",
    "    bert_tokens = bert_tokenizer.tokenize(text)\n",
    "\n",
    "    # Create visualization of attention patterns (conceptual)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # GPT-2 causal attention mask (lower triangular)\n",
    "    gpt2_mask = np.tril(np.ones((len(gpt2_tokens), len(gpt2_tokens))))\n",
    "    ax1.imshow(gpt2_mask, cmap='Blues')\n",
    "    ax1.set_title('GPT-2 Causal Attention Pattern')\n",
    "    ax1.set_xticks(range(len(gpt2_tokens)))\n",
    "    ax1.set_yticks(range(len(gpt2_tokens)))\n",
    "    ax1.set_xticklabels(gpt2_tokens, rotation=90)\n",
    "    ax1.set_yticklabels(gpt2_tokens)\n",
    "\n",
    "    # BERT full attention mask (full matrix)\n",
    "    bert_mask = np.ones((len(bert_tokens), len(bert_tokens)))\n",
    "    ax2.imshow(bert_mask, cmap='Oranges')\n",
    "    ax2.set_title('BERT Full Attention Pattern')\n",
    "    ax2.set_xticks(range(len(bert_tokens)))\n",
    "    ax2.set_yticks(range(len(bert_tokens)))\n",
    "    ax2.set_xticklabels(bert_tokens, rotation=90)\n",
    "    ax2.set_yticklabels(bert_tokens)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZfhBdmgxazI"
   },
   "outputs": [],
   "source": [
    "# Part 6: Task-specific behavior demonstration\n",
    "def demonstrate_model_capabilities():\n",
    "    models = load_pretrained_models()\n",
    "    example_text = \"Natural language processing has been transformed by\"\n",
    "\n",
    "    # GPT-2 next token prediction\n",
    "    gpt2_model, gpt2_tokenizer, _ = models['gpt2']\n",
    "\n",
    "    gpt2_inputs = gpt2_tokenizer(example_text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        gpt2_outputs = gpt2_model(**gpt2_inputs)\n",
    "        gpt2_hidden_states = gpt2_outputs.last_hidden_state\n",
    "\n",
    "    # BERT bidirectional processing\n",
    "    bert_model, bert_tokenizer, _ = models['bert']\n",
    "\n",
    "    bert_inputs = bert_tokenizer(example_text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        bert_outputs = bert_model(**bert_inputs)\n",
    "        bert_hidden_states = bert_outputs.last_hidden_state\n",
    "\n",
    "    # T5 text-to-text framework\n",
    "    t5_model, t5_tokenizer, _ = models['t5']\n",
    "\n",
    "    t5_inputs = t5_tokenizer(example_text, return_tensors=\"pt\")\n",
    "    t5_decoder_inputs = t5_tokenizer(\"the\", return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        t5_outputs = t5_model(input_ids=t5_inputs.input_ids, decoder_input_ids=t5_decoder_inputs.input_ids)\n",
    "        t5_encoder_states = t5_outputs.encoder_last_hidden_state\n",
    "        t5_decoder_states = t5_outputs.last_hidden_state\n",
    "\n",
    "    # Print shapes to show model outputs\n",
    "    print(f\"GPT-2 output shape: {gpt2_hidden_states.shape}\")\n",
    "    print(f\"BERT output shape: {bert_hidden_states.shape}\")\n",
    "    print(f\"T5 encoder output shape: {t5_encoder_states.shape}\")\n",
    "    print(f\"T5 decoder output shape: {t5_decoder_states.shape}\")\n",
    "\n",
    "    return {\n",
    "        'gpt2': gpt2_hidden_states,\n",
    "        'bert': bert_hidden_states,\n",
    "        't5_encoder': t5_encoder_states,\n",
    "        't5_decoder': t5_decoder_states\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1KE2_w0xawr"
   },
   "outputs": [],
   "source": [
    "# Part 7: Comparative analysis\n",
    "def architecture_comparison_table():\n",
    "    # Create a table showing key differences\n",
    "    data = {\n",
    "        'Model': ['GPT (Autoregressive)', 'BERT (Autoencoding)', 'T5 (Seq2Seq)'],\n",
    "        'Architecture': ['Decoder-only', 'Encoder-only', 'Encoder-Decoder'],\n",
    "        'Attention': ['Causal (unidirectional)', 'Bidirectional', 'Bidirectional + Causal'],\n",
    "        'Pre-training': ['Next-token prediction', 'Masked LM + Next Sentence Prediction', 'Text-to-Text'],\n",
    "        'Best suited for': ['Text generation', 'Understanding & Classification', 'Translation & Summarization']\n",
    "    }\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    table = ax.table(\n",
    "        cellText=list(zip(*data.values())),\n",
    "        colLabels=list(data.keys()),\n",
    "        cellLoc='center',\n",
    "        loc='center'\n",
    "    )\n",
    "\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 1.5)\n",
    "\n",
    "    plt.title('Comparison of LLM Architectures')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQIDaRLgxgzx"
   },
   "outputs": [],
   "source": [
    "# Part 8: Implementation of a simple transformer from scratch\n",
    "class SimplifiedTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8, num_encoder_layers=3,\n",
    "                 num_decoder_layers=3, dim_feedforward=1024, dropout=0.1,\n",
    "                 max_seq_length=128, architecture='seq2seq'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.architecture = architecture  # 'decoder_only', 'encoder_only', or 'seq2seq'\n",
    "\n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_length, d_model)\n",
    "\n",
    "        # Encoder (for BERT and T5)\n",
    "        if architecture in ['encoder_only', 'seq2seq']:\n",
    "            encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "            self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "\n",
    "        # Decoder (for GPT and T5)\n",
    "        if architecture in ['decoder_only', 'seq2seq']:\n",
    "            decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "            self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers)\n",
    "\n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        def forward(self, src_tokens, tgt_tokens=None, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "          batch_size, src_len = src_tokens.size()\n",
    "          src_positions = torch.arange(src_len, device=src_tokens.device).unsqueeze(0)\n",
    "          src_emb = self.token_embedding(src_tokens) + self.position_embedding(src_positions)\n",
    "\n",
    "          if self.architecture in ['encoder_only', 'seq2seq']:\n",
    "              memory = self.encoder(src_emb, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "          else:\n",
    "              memory = None\n",
    "\n",
    "          if self.architecture == 'encoder_only':\n",
    "              output = self.output_projection(memory)\n",
    "          else:\n",
    "              if tgt_tokens is None:\n",
    "                  raise ValueError(\"tgt_tokens must be provided for decoder or seq2seq architectures\")\n",
    "              tgt_len = tgt_tokens.size(1)\n",
    "              tgt_positions = torch.arange(tgt_len, device=tgt_tokens.device).unsqueeze(0)\n",
    "              tgt_emb = self.token_embedding(tgt_tokens) + self.position_embedding(tgt_positions)\n",
    "\n",
    "              if self.architecture == 'decoder_only':\n",
    "                  output_states = self.decoder(\n",
    "                      tgt_emb,\n",
    "                      memory if memory is not None else tgt_emb,\n",
    "                      tgt_mask=tgt_mask,\n",
    "                      memory_key_padding_mask=src_key_padding_mask,\n",
    "                      tgt_key_padding_mask=tgt_key_padding_mask\n",
    "                  )\n",
    "              else:  # seq2seq\n",
    "                  output_states = self.decoder(\n",
    "                      tgt_emb,\n",
    "                      memory,\n",
    "                      tgt_mask=tgt_mask,\n",
    "                      memory_key_padding_mask=src_key_padding_mask,\n",
    "                      tgt_key_padding_mask=tgt_key_padding_mask\n",
    "                  )\n",
    "\n",
    "              output = self.output_projection(output_states)\n",
    "\n",
    "          return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSBH7Sys_5Ls"
   },
   "outputs": [],
   "source": [
    "# Run full notebook demonstrations\n",
    "param_counts = model_parameter_comparison()\n",
    "visualize_attention_patterns()\n",
    "outputs = demonstrate_model_capabilities()\n",
    "architecture_comparison_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPjWntj3v9be"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cyn1F_6zv9Ym"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeZfK3DvwIlt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-Sp0sMNwIjE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnVinRV7wI69"
   },
   "source": [
    "# Quiz -1\n",
    "\n",
    "Question 1: Which architecture uses causal masking for autoregressive generation?\n",
    "1. BERT\n",
    "2. GPT (Correct)\n",
    "3. T5\n",
    "4. Transformer (original)\n",
    "\n",
    "\n",
    "Question 2: Which model architecture employs bidirectional context for token prediction?\n",
    "1. GPT-2\n",
    "2. BERT (Correct)\n",
    "3. GPT-3\n",
    "4. PaLM\n",
    "\n",
    "\n",
    "Question 3: What is the key innovation of T5 compared to other architectures?\n",
    "1. Larger parameter count\n",
    "2. Text-to-text framework (Correct)\n",
    "3. Mixture-of-experts\n",
    "4. Reinforcement learning\n",
    "\n",
    "\n",
    "Question 4: Which architecture is best suited for classification tasks?\n",
    "1. GPT\n",
    "2. BERT (Correct)\n",
    "3. T5\n",
    "4. All equally\n",
    "\n",
    "\n",
    "Question 5: What does the \"autoregressive\" property in GPT refer to?\n",
    "1. Self-improving capabilities\n",
    "2. Predicting each token based on previous tokens (Correct)\n",
    "3. Automatic gradient computation\n",
    "4. Recursively generating embeddings\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_rlm1utwe-0"
   },
   "source": [
    "# Quiz -2\n",
    "\n",
    "1. **Which architecture uses causal (unidirectional) masking?**  \n",
    "   A. BERT  \n",
    "   B. GPT (Correct)  \n",
    "   C. T5  \n",
    "   D. Transformer (original)\n",
    "\n",
    "2. **Which pre-training objective is unique to BERT?**  \n",
    "   A. Next-token prediction  \n",
    "   B. Masked Language Modeling (MLM) (Correct)\n",
    "   C. Text-to-text reformulation  \n",
    "   D. Causal language modeling\n",
    "\n",
    "3. **In a GPT model, how is token position information added?**  \n",
    "   A. Through token-type embeddings  \n",
    "   B. Via a separate positional-encoding embedding layer (Correct)\n",
    "   C. By concatenating absolute positions to token IDs  \n",
    "   D. It isn’t—GPT infers order purely from attention\n",
    "\n",
    "4. **What core framework does T5 use?**  \n",
    "   A. Decoder-only  \n",
    "   B. Encoder-only  \n",
    "   C. Encoder-Decoder (seq2seq) (Correct)\n",
    "   D. Hybrid GAN-Transformer\n",
    "\n",
    "5. **T5 reformulates every task as:**  \n",
    "   A. A sequence classification problem  \n",
    "   B. A masked-token prediction problem  \n",
    "   C. A text-to-text generation problem (Correct) \n",
    "   D. A next-sentence prediction problem\n",
    "\n",
    "6. **Which architecture is best suited for pure text generation tasks (e.g., writing a story)?**  \n",
    "   A. BERT  \n",
    "   B. GPT (Correct)\n",
    "   C. T5  \n",
    "   D. All are equally good\n",
    "\n",
    "7. **True or False:** BERT’s self-attention blocks allow each token to attend only to previous tokens.  \n",
    "   - True  \n",
    "   - False (False)\n",
    "\n",
    "8. **Short answer:** Name one real-world application powered by GPT-style models.\n",
    "\n",
    "Chatbots, such as ChatGPT, used for customer support, virtual assistants, and interactive AI companions.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPjt2P5dpH3c8H8/NOPSvZh",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
