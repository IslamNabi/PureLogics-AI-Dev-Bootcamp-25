{"cells":[{"cell_type":"markdown","metadata":{"id":"2iHaEO4PLldZ"},"source":["#Prompt Engineering Notebook Exercise using Gemini Flash 1.5\n","\n","\n","##Welcome to this interactive notebook on Prompt Engineering, where you'll learn how to craft effective prompts for generative AI using Google's Gemini 1.5 Flash model.\n","\n","Prompt engineering is the art of designing inputs (prompts) that guide a large language model (LLM) to generate accurate, relevant, and context-aware responses. It's an essential skill for developers, researchers, and AI enthusiasts looking to build smarter, more controllable AI applications.\n","\n","In this notebook, you will:\n","\n","üîë Use your Gemini API Key to access Google's latest Gemini 1.5 Flash model.\n","\n","üß™ Experiment with various prompt types including:\n","\n","Instruction-based prompts\n","\n","Summarization and rephrasing\n","\n","Creative generation\n","\n","Few-shot examples\n","\n","Classification and zero-shot tasks"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"L0ssMdNzLxKj","executionInfo":{"status":"ok","timestamp":1752046635768,"user_tz":-300,"elapsed":7003,"user":{"displayName":"IslamNabi","userId":"15268998542759672787"}}},"outputs":[],"source":["pip install -q -U google-generativeai"]},{"cell_type":"markdown","metadata":{"id":"3PvVBa5ROOYx"},"source":["#üîê How to Get Your Gemini API Key\n","To use the Gemini 1.5 Flash model in this notebook, you‚Äôll need a free API key from Google AI Studio. Follow the steps below to generate one:\n","\n","Step-by-Step Instructions:\n","\n","1. Visit Google AI Studio\n","\n","  Open this link in your browser: https://aistudio.google.com/app/apikey\n","\n","2. Sign in with Your Google Account\n","\n","  Make sure you're signed in with a Google account (Gmail, Workspace, etc.).\n","\n","3. Agree to Terms\n","\n","  If it's your first time using AI Studio, you'll be asked to accept the terms and conditions.\n","\n","4. Generate an API Key\n","\n","  Click the ‚ÄúCreate API Key‚Äù button.\n","\n","5. A key will be displayed. It will look something like this:\n","AI...123xyz\n","\n","6. Copy Your API Key\n","\n","  Make sure to copy and store it somewhere safe.\n","\n","7. You‚Äôll paste this key into the notebook cell below."]},{"cell_type":"code","source":["import google.generativeai as genai\n","import getpass\n","\n","api_key = getpass.getpass(\"Enter your Gemini API Key: \")\n","genai.configure(api_key=api_key)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"10gjdbDYIzA8","executionInfo":{"status":"ok","timestamp":1752047080215,"user_tz":-300,"elapsed":9449,"user":{"displayName":"IslamNabi","userId":"15268998542759672787"}},"outputId":"7c100824-aa0a-4455-f0e2-ff67048d7535"},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your Gemini API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"gZEthIP0GZBK","executionInfo":{"status":"ok","timestamp":1752047106573,"user_tz":-300,"elapsed":8,"user":{"displayName":"IslamNabi","userId":"15268998542759672787"}}},"outputs":[],"source":["model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash-latest\")"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Wg-s3nnLGfrU","executionInfo":{"status":"ok","timestamp":1752047124716,"user_tz":-300,"elapsed":25,"user":{"displayName":"IslamNabi","userId":"15268998542759672787"}}},"outputs":[],"source":["prompt = \"Summarize the concept of reinforcement learning in AI.\""]},{"cell_type":"code","execution_count":8,"metadata":{"id":"yLB-75_zGmpB","colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"status":"ok","timestamp":1752047152720,"user_tz":-300,"elapsed":7528,"user":{"displayName":"IslamNabi","userId":"15268998542759672787"}},"outputId":"b1a84022-c9c0-4a15-bf5d-3723a27c7ec6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reinforcement learning (RL) is a type of machine learning where an agent learns to interact with an environment by taking actions and receiving rewards or penalties.  The goal of the agent is to learn a *policy*, which is a strategy for selecting actions that maximizes its cumulative reward over time.  This learning process happens through trial and error, with the agent adjusting its policy based on the consequences of its actions.  Unlike supervised learning which uses labeled data, RL learns from interaction and feedback in the form of rewards.  Think of it like training a dog: you reward good behavior (positive reinforcement) and correct bad behavior (negative reinforcement), eventually shaping the dog's actions to what you want.\n","\n"]}],"source":["response = model.generate_content(prompt)\n","print(response.text)"]},{"cell_type":"markdown","metadata":{"id":"ORVPLAYQkLIY"},"source":["#General Query"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"oAS44keMLxKm","colab":{"base_uri":"https://localhost:8080/","height":634},"executionInfo":{"status":"ok","timestamp":1752047234975,"user_tz":-300,"elapsed":6868,"user":{"displayName":"IslamNabi","userId":"15268998542759672787"}},"outputId":"bc400eaf-826b-4afb-e38c-2a7969201f4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt engineering is the art and science of designing effective prompts to elicit desired outputs from language models (LLMs) like GPT-3, LaMDA, etc.  Its basic concepts revolve around understanding how to communicate effectively with these models, maximizing their strengths, and minimizing their weaknesses.  Here are some core concepts:\n","\n","**1. Clarity and Specificity:**\n","\n","* **Unambiguous Instructions:**  Avoid vague terms or jargon. Be explicit about what you want the model to do. Instead of \"Write something about dogs,\" try \"Write a 200-word essay comparing the temperaments of Golden Retrievers and German Shepherds.\"\n","* **Defined Output Format:** Specify the desired format (e.g., list, paragraph, poem, code, JSON).  For example, \"List five benefits of exercise, each in a bullet point.\"\n","* **Length Constraints:**  Indicate the desired length (word count, number of items, etc.) to guide the model's response.\n","\n","**2. Context and Examples:**\n","\n","* **Providing Context:** Give the model sufficient background information to understand your request.  This might include relevant keywords, definitions, or examples.\n","* **Few-Shot Learning:** Provide a few examples of the input-output pairs you expect. This helps the model understand the task better.  For instance, if you want the model to translate phrases, provide a few examples of English phrases and their French translations before giving it a new phrase to translate.\n","\n","**3. Controlling the Model's Style and Tone:**\n","\n","* **Style Directives:** Specify the desired style (e.g., formal, informal, humorous, persuasive, technical).  For example, \"Write a formal letter requesting a refund.\"\n","* **Tone Directives:** Indicate the desired tone (e.g., optimistic, pessimistic, serious, playful). For example, \"Write a humorous short story about a cat.\"\n","* **Persona Directives:**  Specify a persona for the model to adopt (e.g., a historical figure, a fictional character).  For example, \"Write a tweet as if you were Elon Musk.\"\n","\n","**4. Iteration and Refinement:**\n","\n","* **Experimentation:**  Try different prompts and variations to see what works best.  Don't be afraid to experiment with different phrasing, keywords, and examples.\n","* **Analyzing Outputs:** Carefully examine the model's responses to identify areas for improvement in your prompts.\n","* **Iterative Process:** Prompt engineering is an iterative process.  You'll likely need to refine your prompts multiple times to achieve the desired results.\n","\n","**5. Understanding Model Limitations:**\n","\n","* **Hallucinations:** LLMs can sometimes generate incorrect or nonsensical information.  Be aware of this and verify the information provided by the model.\n","* **Bias:** LLMs can reflect biases present in their training data.  Be mindful of potential biases in the model's outputs.\n","* **Context Window:** LLMs have a limited context window, meaning they can only process a certain amount of text at a time.  Keep your prompts concise and focused.\n","\n","By mastering these basic concepts, you can significantly improve the quality and relevance of the outputs generated by language models.  It's a continuous learning process of understanding both the capabilities and limitations of the specific model you are using.\n","\n"]}],"source":["response = model.generate_content([\"What are the basic concepts of prompt engineering?\"])\n","print(response.text)"]},{"cell_type":"markdown","metadata":{"id":"8yaad_A8kOuP"},"source":["Detailed response"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"eDiMyRkPLxKm","colab":{"base_uri":"https://localhost:8080/","height":634},"executionInfo":{"status":"ok","timestamp":1752047353992,"user_tz":-300,"elapsed":6772,"user":{"displayName":"IslamNabi","userId":"15268998542759672787"}},"outputId":"c92157f0-a100-461e-b638-1ee531c069b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Several innovations are pushing the boundaries of long-context LLMs.  It's difficult to give a perfectly exhaustive list, as research in this area is rapidly evolving, but here are some notable examples categorized by approach:\n","\n","**I.  Improving Attention Mechanisms:**  These methods aim to make the attention mechanism itself more efficient for longer sequences.\n","\n","* **Title:**  Longformer: The Long-Document Transformer\n","* **Summary:** This paper introduces the Longformer architecture, which modifies the self-attention mechanism to handle longer sequences more efficiently. It uses a combination of sliding window attention and global attention to achieve this.  It's more efficient than standard Transformer attention for long sequences without sacrificing performance significantly.\n","\n","* **Title:**  Reformer: The Efficient Transformer\n","* **Summary:** The Reformer utilizes locality-sensitive hashing (LSH) for attention, reducing the quadratic complexity of standard attention to near-linear. This allows processing much longer sequences than standard Transformers.  It uses other techniques like reversible layers to further improve efficiency.\n","\n","* **Title:**  Linformer: Self-Attention with Linear Complexity\n","* **Summary:** Linformer proposes a linear-complexity attention mechanism.  It projects the keys and values into a lower-dimensional space before performing attention, significantly reducing the computational cost. This allows it to handle much longer sequences than standard Transformers.\n","\n","\n","**II.  Memory Augmentation:**  These techniques augment the LLM with external memory to store and retrieve information beyond the immediate context window.\n","\n","* **Title:**  Retrieval-Augmented Generation (RAG) (Not a single paper, but a common approach)\n","* **Summary:** RAG methods combine LLMs with external knowledge bases (like vector databases).  The LLM retrieves relevant information from the knowledge base based on the input prompt, and then uses this information alongside the prompt to generate a response.  This allows the model to access information exceeding its context window.  Many papers explore different aspects of RAG, focusing on retrieval efficiency and integration with LLMs.\n","\n","\n","**III.  Hierarchical Architectures:**  These approaches organize information hierarchically to better manage long sequences.\n","\n","* **Title:**  Many papers explore hierarchical architectures, but there isn't one single dominant paper.\n","* **Summary:**  These models often involve breaking down long documents into smaller chunks, processing them individually, and then aggregating the results to create a coherent representation. This can involve various techniques, such as recurrent neural networks acting on the outputs of chunk-level processing or using tree-structured models.\n","\n","\n","**IV.  Mixture-of-Experts (MoE) Models:**  These distribute the processing of long sequences across multiple experts.\n","\n","* **Title:**  GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\n","* **Summary:** GShard describes a technique for training extremely large models, including those that use MoE.  By assigning different parts of the input sequence to different expert networks, MoE approaches can handle longer sequences more efficiently than a single large model.  They selectively route information to relevant experts, avoiding unnecessary computation.\n","\n","**Important Note:**  The \"best\" method depends on the specific application and resource constraints.  Many recent works combine several of these approaches for optimal performance. For instance, a model might use a combination of improved attention mechanisms and retrieval augmentation to handle exceptionally long contexts.  The field is very active, so always check for recent publications on arXiv or similar resources for the most up-to-date innovations.\n","\n"]}],"source":["response = model.generate_content([\"What are some of the innovations around long context LLMs? List the title of the paper and summary.\"])\n","print(response.text)"]},{"cell_type":"markdown","metadata":{"id":"0miQLIjgkSQr"},"source":["#Summarization Prompt"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"kfraYOnRLxKm","colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"status":"ok","timestamp":1752047439509,"user_tz":-300,"elapsed":6311,"user":{"displayName":"IslamNabi","userId":"15268998542759672787"}},"outputId":"6979dd84-d3aa-4c53-fa18-506c661026ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["AI is the general concept of machines performing smart tasks.  Machine learning (ML) is a specific type of AI, and deep learning (DL) is a specialized form of ML using multi-layered neural networks.  In short: AI > ML > DL.\n","\n"]}],"source":["long_text = \"\"\"\n","Artificial Intelligence (AI) refers to the broader concept of machines being able to carry out tasks in a way that we would consider ‚Äúsmart‚Äù.\n","Machine Learning (ML) is a subset of AI, and Deep Learning (DL) is a subset of ML. DL uses neural networks with multiple layers.\n","\"\"\"\n","response = model.generate_content([f\"Summarize the following:\\n\\n{long_text}\"])\n","print(response.text)"]},{"cell_type":"markdown","metadata":{"id":"gcoO8ZHVkUnd"},"source":["#Rephrasing Prompt"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Y6lmMkGbLXZo","colab":{"base_uri":"https://localhost:8080/","height":144},"executionInfo":{"status":"ok","timestamp":1752047489038,"user_tz":-300,"elapsed":3019,"user":{"displayName":"IslamNabi","userId":"15268998542759672787"}},"outputId":"109742f7-9096-4bd9-aa23-18a0836176df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Here are a few ways to rephrase the sentence, each with a slightly different emphasis:\n","\n","* **Option 1 (More concise):** Modern AI practitioners need strong prompt engineering skills.\n","* **Option 2 (Emphasis on importance):** Mastering prompt engineering is crucial for success in modern AI.\n","* **Option 3 (Slightly more formal):**  Prompt engineering is a vital competency for those working with modern AI systems.\n","* **Option 4 (Focus on the future):**  Prompt engineering will be an indispensable skill for future AI professionals.\n","\n"]}],"source":["response = model.generate_content([\"Rephrase the sentence: 'Prompt engineering is an essential skill for modern AI practitioners.'\"])\n","print(response.text)"]},{"cell_type":"markdown","metadata":{"id":"4P97-o2AkXnZ"},"source":["#Few-shot Classification Prompt"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"2_qCH1ysLZUh","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1752047569690,"user_tz":-300,"elapsed":1601,"user":{"displayName":"IslamNabi","userId":"15268998542759672787"}},"outputId":"bf5dc032-b572-4297-acb1-ec53f73c5145"},"outputs":[{"output_type":"stream","name":"stdout","text":["Negative.  While not strongly negative, the sentence expresses disappointment and falls short of positive sentiment.\n","\n"]}],"source":["prompt = \"\"\"\n","Classify the sentiment of each sentence as Positive, Neutral, or Negative.\n","\n","1. I love this new feature! ‚Üí Positive\n","2. The app is okay, nothing special. ‚Üí Neutral\n","3. This is the worst experience ever. ‚Üí Negative\n","4. I guess it works, but I expected better. ‚Üí\n","\"\"\"\n","response = model.generate_content([prompt])\n","print(response.text)"]},{"cell_type":"markdown","metadata":{"id":"68OmWiDKkeOe"},"source":["#Creative Prompt / Idea Generation"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"OiLU8iT3LbyZ","colab":{"base_uri":"https://localhost:8080/","height":126},"executionInfo":{"status":"ok","timestamp":1752047597570,"user_tz":-300,"elapsed":2199,"user":{"displayName":"IslamNabi","userId":"15268998542759672787"}},"outputId":"cb9cf266-323d-45b3-8044-7201d1d74a11"},"outputs":[{"output_type":"stream","name":"stdout","text":["1. **Unlocking AI's Potential: Your Beginner's Guide to Prompt Engineering Wizardry** (Emphasizes empowerment and a touch of playful magic)\n","2. **From Zero to AI Hero: Mastering the Art of the Perfect Prompt** (Focuses on transformation and achievable mastery)\n","3. **Talking to Machines: A Beginner's Handbook for Effective Prompt Engineering** (Uses relatable language and clearly defines the topic)\n","4. **Beyond \"Write a Poem\":  Unleashing the Creative Power of Prompt Engineering** (Highlights the limitations of basic prompts and promises more advanced techniques)\n","5. **Demystifying Prompt Engineering: Simple Steps to Amazing AI Results** (Direct, promises practical results, and uses the word \"simple\" to appeal to beginners)\n","\n"]}],"source":["response = model.generate_content([\"Give me 5 creative blog title ideas about prompt engineering for beginners.\"])\n","print(response.text)"]},{"cell_type":"markdown","metadata":{"id":"jDtJwGpukhbj"},"source":["#Zero-shot Task ‚Äî Generate Code"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"UDfathRWLdWf","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1752047627892,"user_tz":-300,"elapsed":5411,"user":{"displayName":"IslamNabi","userId":"15268998542759672787"}},"outputId":"5191730b-3266-4056-ffc0-329ad904fc3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Several ways exist to reverse a string in Python. Here are three options, with explanations:\n","\n","**Method 1: Slicing**\n","\n","This is the most Pythonic and efficient way:\n","\n","```python\n","def reverse_string_slicing(s):\n","  \"\"\"Reverses a string using slicing.\n","\n","  Args:\n","    s: The input string.\n","\n","  Returns:\n","    The reversed string.\n","  \"\"\"\n","  return s[::-1]\n","\n","#Example\n","string = \"hello\"\n","reversed_string = reverse_string_slicing(string)\n","print(f\"Reversed string: {reversed_string}\") # Output: Reversed string: olleh\n","```\n","\n","Slicing `[::-1]` creates a reversed copy of the string without modifying the original.  It's concise and readable.\n","\n","\n","**Method 2: `reversed()` function and `join()` method**\n","\n","This method uses the built-in `reversed()` function, which returns an iterator of characters in reverse order, and then joins them back into a string:\n","\n","```python\n","def reverse_string_reversed(s):\n","  \"\"\"Reverses a string using the reversed() function and join().\n","\n","  Args:\n","    s: The input string.\n","\n","  Returns:\n","    The reversed string.\n","  \"\"\"\n","  return \"\".join(reversed(s))\n","\n","#Example\n","string = \"hello\"\n","reversed_string = reverse_string_reversed(string)\n","print(f\"Reversed string: {reversed_string}\") # Output: Reversed string: olleh\n","```\n","\n","This is also efficient and relatively clear.\n","\n","\n","**Method 3: Looping (less efficient)**\n","\n","This approach uses a loop to iterate through the string from the end to the beginning and build the reversed string:\n","\n","```python\n","def reverse_string_loop(s):\n","  \"\"\"Reverses a string using a loop.\n","\n","  Args:\n","    s: The input string.\n","\n","  Returns:\n","    The reversed string.\n","  \"\"\"\n","  reversed_s = \"\"\n","  for i in range(len(s) - 1, -1, -1):\n","    reversed_s += s[i]\n","  return reversed_s\n","\n","#Example\n","string = \"hello\"\n","reversed_string = reverse_string_loop(string)\n","print(f\"Reversed string: {reversed_string}\") # Output: Reversed string: olleh\n","```\n","\n","While this works, it's generally less efficient than slicing or using `reversed()` and `join()` because string concatenation in a loop creates many intermediate string objects.  Avoid this method for performance-critical applications.\n","\n","\n","For most cases, the **slicing method (`[::-1]`) is recommended** due to its conciseness and efficiency.  The `reversed()` and `join()` method is a good alternative if you prefer a more explicit approach.  Avoid the looping method unless you have a specific reason to use it.\n","\n"]}],"source":["response = model.generate_content([\"Write a Python function to reverse a string.\"])\n","print(response.text)"]},{"cell_type":"markdown","metadata":{"id":"WRu8jGFoYjQu"},"source":["#Test your knowledge\n","\n","Below are a few exercises where you can modify the weak prompts to generate desired results"]},{"cell_type":"markdown","metadata":{"id":"1zHRhnl_YxUS"},"source":["# Exercise 1: Prompt Refinement & Comparison\n","Goal: Craft two different prompts for the same task, compare outputs, and iteratively refine for clarity.\n","\n","Task: ‚ÄúGenerate a 3‚Äëpoint checklist for secure password policies.‚Äù\n","\n","Steps:\n","\n","1. Write Prompt A (basic instruction).\n","\n","2. Write Prompt B (include context or examples).\n","\n","3. Call Gemini with both.\n","\n","4. Analyze differences: completeness, specificity, style.\n","\n","5. Refine the weaker prompt and re‚Äëtest."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"oEWmm8sNY3AW","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1752048017688,"user_tz":-300,"elapsed":13719,"user":{"displayName":"IslamNabi","userId":"15268998542759672787"}},"outputId":"961cd678-83c2-4c0c-e6c2-8da1c8efbd3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== Prompt A ===\n","## Secure Password Policy Checklist: 3 Key Points\n","\n","This checklist ensures strong and secure passwords across your organization or personal accounts.\n","\n","**1.  Length and Complexity:**\n","\n","* **Requirement:** Passwords must be at least 12 characters long and include a mix of uppercase and lowercase letters, numbers, and symbols.  Avoid using easily guessable sequences like \"123456\" or repeating characters.\n","* **Example (Good):**  `P@$$wOrd!2023`  (14 characters, uppercase, lowercase, numbers, symbols)\n","* **Example (Bad):** `Password1` (too short, lacks complexity)\n","* **Example (Bad):** `Aa1Bb2Cc3` (predictable pattern)\n","* **Action:**  Implement password complexity requirements in all systems and enforce them through account creation and password change processes.  Consider using a password manager to generate and store complex passwords securely.\n","\n","\n","**2.  Uniqueness and Rotation:**\n","\n","* **Requirement:**  Use unique passwords for each online account.  Avoid reusing passwords across different websites or services. Regularly change passwords, ideally every 90 days or sooner if there is a suspected breach.\n","* **Example (Good):** Different passwords for email, banking, social media, etc.\n","* **Example (Bad):** Using the same password for all accounts.\n","* **Action:**  Educate users on the importance of password uniqueness and implement mandatory password change policies with reminders.  Encourage the use of password managers to simplify this process.\n","\n","\n","**3.  Protection Against Phishing and Brute-Force Attacks:**\n","\n","* **Requirement:**  Enable multi-factor authentication (MFA) wherever possible.  Be wary of phishing emails and suspicious links that request login credentials. Regularly monitor accounts for suspicious activity.\n","* **Example (Good):** Using Google Authenticator or similar apps for MFA on email and other important accounts.\n","* **Example (Bad):** Relying solely on passwords for account security.\n","* **Action:**  Implement MFA across all critical systems and educate users about phishing techniques and best practices for identifying suspicious communications.  Set up account monitoring alerts for unusual login attempts or password changes.\n","\n","\n","=== Prompt B ===\n","As a cybersecurity consultant, I'd recommend these three detailed best practices for password security:\n","\n","**1.  Employ Strong and Unique Passwords Utilizing a Password Manager:**\n","\n","* **Strength:**  Avoid easily guessable passwords.  A strong password should meet these criteria:\n","    * **Length:** At least 16 characters. Longer is better.\n","    * **Complexity:** Include a mix of uppercase and lowercase letters, numbers, and symbols. Avoid easily-guessed patterns like sequential numbers (1234) or keyboard patterns (qwerty).  Consider using a passphrase (a memorable sentence turned into a password) as a strong alternative, but still aim for 16+ characters.\n","    * **Predictability:**  Do not use personal information like birthdays, anniversaries, pet names, or common words found in dictionaries.  Avoid variations on previous passwords.\n","\n","* **Uniqueness:** Every account should have a unique password.  If one account is compromised, attackers won't have access to your other accounts. This is critical.\n","\n","* **Password Manager:** The most effective way to manage strong and unique passwords is with a reputable password manager. These tools generate strong passwords, securely store them, and automatically fill them in when you log in. Choose a password manager with strong security features like end-to-end encryption, two-factor authentication (2FA), and a robust security audit track record.  Consider reputable options like Bitwarden (open-source), 1Password, LastPass, or Dashlane, carefully researching their security practices before committing.  Do *not* rely on your browser's built-in password manager as its security is often less robust.\n","\n","**2.  Enable and Utilize Multi-Factor Authentication (MFA) Whenever Possible:**\n","\n","* **Implementation:** MFA adds an extra layer of security beyond just a password. It requires a second form of authentication to access an account, even if the password is compromised.  Common methods include:\n","    * **Time-based One-Time Password (TOTP):**  Uses an app like Google Authenticator or Authy to generate a code that changes every 30 seconds.\n","    * **SMS/Text Message:** Receives a code via text message. While convenient, SMS-based MFA is becoming increasingly vulnerable to SIM swapping attacks, making it a less preferred method.\n","    * **Security Key/Hardware Token:** A physical device (USB or NFC) that generates a unique code for authentication. This is generally the most secure option.\n","    * **Biometrics:** Fingerprint, facial recognition, or other biometric authentication. While convenient, these can be vulnerable to spoofing attacks, depending on the implementation.\n","\n","* **Application:**  Enable MFA on all critical accounts, including email, banking, social media, and cloud services.  Don't hesitate to use different MFA methods across accounts for added security.  Prioritize security key/hardware token options where available.\n","\n","**3.  Practice Good Password Hygiene and Stay Vigilant:**\n","\n","* **Regular Password Changes:** While not as crucial as strong, unique passwords and MFA, regularly changing passwords on high-risk accounts (like email and banking) can help mitigate risk if a breach occurs. Aim for changes every 3-6 months, or as per the service provider's recommendations.  Utilize your password manager to simplify this process.\n","\n","* **Password Reuse Detection:** Utilize the features of your password manager or specialized security tools to scan for password reuse across your accounts.  This alerts you if you‚Äôve accidentally used the same password on multiple services.\n","\n","* **Phishing Awareness:** Be vigilant against phishing attempts. Never click on suspicious links or download attachments from unknown senders. Always verify the sender's identity before entering any login credentials.  Educate yourself on common phishing tactics and red flags.\n","\n","* **Compromise Monitoring:** Regularly monitor your accounts for any unauthorized activity.  Enable account security alerts, which many services offer, to receive notifications about suspicious login attempts or changes to your account settings.  Consider using a credit monitoring service to detect potential identity theft.\n","\n","\n","By diligently following these best practices, individuals and organizations can significantly enhance their password security posture and minimize the risk of data breaches and cyberattacks. Remember that security is a layered approach, and these practices should be implemented in conjunction with other security measures.\n","\n","\n"]}],"source":["# Starter code\n","prompts = {\n","    \"A\": \"Create a clear and actionable 3-point checklist for secure password policies, including examples.\",\n","    \"B\": \"As a cybersecurity consultant, list 3 detailed best practices for password security.\"\n","}\n","\n","for label, prompt in prompts.items():\n","    resp = model.generate_content([prompt])\n","    print(f\"=== Prompt {label} ===\\n{resp.text}\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"lLS-32olZBB5"},"source":["#Exercise 2: Few‚ÄëShot Classification\n","-> Goal: Build a sentiment classifier using few‚Äëshot examples within the prompt.\n","\n","1. Provide 4 labeled sentences (Positive/Negative).\n","\n","2. Append 2 new unlabeled sentences.\n","\n","3. Parse the raw text output into a Python dict.\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"a5ISzWGbZu3s","colab":{"base_uri":"https://localhost:8080/","height":144},"executionInfo":{"status":"ok","timestamp":1752048448680,"user_tz":-300,"elapsed":7550,"user":{"displayName":"IslamNabi","userId":"15268998542759672787"}},"outputId":"16b38354-aa76-40b1-bff5-12d84effb890"},"outputs":[{"output_type":"stream","name":"stdout","text":["Raw output:\n"," 5. The support team was very helpful. ‚Üí **Positive**\n","6. The response time is always slow. ‚Üí **Negative**\n","\n","\n","Extracted Labels:\n","{'5': 'Positive', '6': 'Negative'}\n"]}],"source":["# Starter code\n","few_shot_prompt = \"\"\"\n","Classify each sentence as Positive or Negative:\n","\n","1. I love the new interface! ‚Üí Positive\n","2. The update ruined my workflow. ‚Üí Negative\n","3. This integration makes my life easier. ‚Üí Positive\n","4. I can't access the dashboard anymore. ‚Üí Negative\n","5. The support team was very helpful. ‚Üí\n","6. The response time is always slow. ‚Üí\n","\"\"\"\n","\n","resp = model.generate_content([few_shot_prompt])\n","print(\"Raw output:\\n\", resp.text)\n","\n","# Next: write code to extract labels for sentences 5 & 6 and save them in a dictionary\n","\n","import re  # ensure re is imported\n","\n","labels = {}\n","\n","# This pattern allows for optional ** around the label\n","matches = re.findall(r\"5\\..*‚Üí\\s*\\*{0,2}(Positive|Negative)\\*{0,2}\", resp.text)\n","if matches:\n","    labels[\"5\"] = matches[0]\n","\n","matches = re.findall(r\"6\\..*‚Üí\\s*\\*{0,2}(Positive|Negative)\\*{0,2}\", resp.text)\n","if matches:\n","    labels[\"6\"] = matches[0]\n","\n","print(\"\\nExtracted Labels:\")\n","print(labels)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NM_Cqrtuadj_"},"source":["# Exercise 3: Multi‚ÄëTurn Context Retention\n","Goal: Build a simple chat loop that retains the last two user messages as context.\n","\n","1. Initialize a chat session.\n","\n","2. After each user input, send the previous two user messages + the new one to Gemini.\n","\n","3. Print the assistant‚Äôs reply."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"SbbioEu-afkz","executionInfo":{"status":"ok","timestamp":1752048555747,"user_tz":-300,"elapsed":32,"user":{"displayName":"IslamNabi","userId":"15268998542759672787"}}},"outputs":[],"source":["# Starter code\n","chat = model.start_chat()\n","history = []\n","\n","def ask(prompt: str):\n","    history.append(prompt)\n","    # keep only the last 2 user turns\n","    context = history[-2:]\n","    messages = [m for m in context]\n","    chat.send_message(messages)\n","    return chat.history[-1].parts[0].text\n"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"QRe14Hx_bDbp","colab":{"base_uri":"https://localhost:8080/","height":272},"executionInfo":{"status":"ok","timestamp":1752048567138,"user_tz":-300,"elapsed":7040,"user":{"displayName":"IslamNabi","userId":"15268998542759672787"}},"outputId":"309212f8-27a9-4846-8f21-9c06805b44bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt engineering is the process of designing and crafting effective prompts to elicit desired outputs from language models (like ChatGPT, Bard, etc.) or other AI systems.  It's about understanding how to communicate your needs clearly and concisely to the AI so it can generate the most relevant, accurate, and useful response.\n","\n","It goes beyond simply typing a question; it involves:\n","\n","* **Understanding the model's capabilities and limitations:**  Knowing what a specific model excels at and where it might struggle is crucial for crafting effective prompts.\n","* **Structuring the prompt effectively:**  This includes using clear instructions, providing context, specifying the desired format (e.g., bullet points, essay, code), and setting constraints (e.g., word count, tone).\n","* **Iterative refinement:**  Experimenting with different prompt phrasing and structures to optimize the output is a key part of the process.\n","* **Using specific keywords and phrases:**  Certain words and phrases can significantly influence the model's response.\n","* **Controlling the style and tone:**  Prompts can guide the AI to adopt a specific style (e.g., formal, informal, humorous) and tone (e.g., optimistic, pessimistic, neutral).\n","* **Few-shot learning:**  Providing examples in the prompt to guide the model's behaviour.\n","\n","In essence, prompt engineering is the art of \"talking\" to AI in a way that maximizes its potential and yields the best possible results.  It's a rapidly evolving field, as new models and techniques are constantly emerging.\n","\n"]}],"source":["# Example usage\n","print(ask(\"Hi, what is prompt engineering?\"))"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"T2JD-_Q1a9w1","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1752048581364,"user_tz":-300,"elapsed":8063,"user":{"displayName":"IslamNabi","userId":"15268998542759672787"}},"outputId":"c0366387-790b-42b4-9378-18ecf50c476e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt engineering is the art and science of designing effective prompts to elicit desired outputs from language models (LLMs).  For summarization, this means crafting prompts that guide the LLM to generate concise, accurate, and informative summaries of given text.\n","\n","To improve your prompts for summarization, consider these strategies:\n","\n","**1. Specify the desired length and style:**\n","\n","* **Length:** Instead of a vague request like \"Summarize this text,\" be specific: \"Summarize this text in 50 words,\" or \"Provide a concise, one-paragraph summary.\"\n","* **Style:**  Indicate the desired tone and style: \"Summarize this article in a formal, academic style,\" or \"Provide a brief, informal summary.\"\n","\n","**2. Specify the focus:**\n","\n","* **Key aspects:**  If the text covers multiple topics, guide the LLM to focus on specific aspects: \"Summarize the article focusing on the economic impact of the policy changes.\"\n","* **Target audience:**  Tailor the summary to a specific audience: \"Summarize this scientific paper for a general audience,\" or \"Summarize this legal document for a non-lawyer.\"\n","\n","**3. Provide context:**\n","\n","* **Background information:** If the text relies on prior knowledge, provide some context: \"Given that the reader is unfamiliar with X, summarize this article on Y.\"\n","* **Source details:**  Mentioning the source can help the LLM understand the context and potential biases: \"Summarize this news article from the New York Times.\"\n","\n","**4. Use keywords and constraints:**\n","\n","* **Keywords:**  Include relevant keywords to guide the summarization process: \"Summarize this research paper, highlighting the keywords 'machine learning' and 'natural language processing'.\"\n","* **Constraints:**  Specify what to include or exclude: \"Summarize this article, excluding details about the author's personal life.\"\n","\n","**5. Employ few-shot learning:**\n","\n","* **Provide examples:**  Give the LLM a few examples of good summaries before presenting the text you want summarized. This helps it understand the desired output format.  For example:\n","\n","```\n","Summarize the following texts:\n","\n","Text 1: \"The quick brown fox jumps over the lazy dog.\"\n","Summary 1: \"A fox leaps over a sleeping dog.\"\n","\n","Text 2: \"The sun is shining brightly today.\"\n","Summary 2: \"It's a sunny day.\"\n","\n","Text 3: [Your text to be summarized]\n","Summary 3:\n","```\n","\n","**6. Iterative refinement:**\n","\n","* **Experiment:** Try different prompt variations to see what works best.  Analyze the outputs and adjust your prompts accordingly.\n","* **Test and adjust:**  Don't be afraid to experiment with different phrasing and instructions.  The best prompt will often depend on the specific LLM and the text being summarized.\n","\n","\n","**Example of a well-crafted prompt:**\n","\n","\"Summarize the following scientific article (attached) in 150 words or less, focusing on the methodology and key findings.  Target audience: undergraduate biology students.  Please use a formal, academic tone and avoid jargon where possible.\"\n","\n","\n","By incorporating these strategies, you can significantly improve the quality and relevance of the summaries generated by language models. Remember to always critically evaluate the output and ensure it accurately reflects the original text.\n","\n"]}],"source":["print(ask(\"How do I improve prompts for summarization?\"))"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"7WcWSS3Fa_2_","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1752048612522,"user_tz":-300,"elapsed":7641,"user":{"displayName":"IslamNabi","userId":"15268998542759672787"}},"outputId":"53badc52-3fb3-443c-97bd-8f1d161859ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["I can't provide a code example in the traditional sense, because prompt engineering isn't directly coded; it's about crafting the *input* to a code-based system (like a large language model API).  There's no \"prompt engineering\" code; the code is within the LLM itself.  Instead, I can show you how to structure your prompts within a code *context* using Python and a hypothetical LLM API call.\n","\n","This example uses a placeholder for an actual LLM API call (like OpenAI's `openai.Completion.create()` or similar).  Replace `\"YOUR_API_KEY\"` and `\"YOUR_LLM_MODEL\"` with your actual API key and model name.\n","\n","\n","```python\n","import openai # Or other LLM library\n","\n","openai.api_key = \"YOUR_API_KEY\"\n","\n","def summarize_text(text, length=150, style=\"formal\", focus=\"\", keywords=\"\"):\n","    \"\"\"\n","    Summarizes text using a large language model.\n","\n","    Args:\n","        text: The text to be summarized.\n","        length: The desired length of the summary (in words).\n","        style: The desired style of the summary (e.g., \"formal\", \"informal\").\n","        focus: The specific aspect to focus on in the summary.\n","        keywords: Keywords to highlight in the summary.\n","\n","    Returns:\n","        The generated summary, or None if there's an error.\n","    \"\"\"\n","    prompt = f\"\"\"Summarize the following text:\n","\n","    {text}\n","\n","    Summary parameters:\n","    * Length: {length} words\n","    * Style: {style}\n","    * Focus: {focus}\n","    * Keywords: {keywords}\n","\n","    Summary:\"\"\"\n","\n","    try:\n","        response = openai.Completion.create(\n","            engine=\"YOUR_LLM_MODEL\",  # Replace with your LLM model\n","            prompt=prompt,\n","            max_tokens=length,  # Adjust based on your model and desired length\n","            n=1,\n","            stop=None,\n","            temperature=0.5, # Adjust for creativity vs. accuracy\n","        )\n","        summary = response.choices[0].text.strip()\n","        return summary\n","    except Exception as e:\n","        print(f\"Error generating summary: {e}\")\n","        return None\n","\n","\n","# Example usage:\n","text_to_summarize = \"\"\"This is a long text that needs summarizing.  It contains many details about various topics, including history, science, and current events.  The text is quite lengthy and covers a broad range of subjects. We need a concise and focused summary.\"\"\"\n","\n","summary = summarize_text(text_to_summarize, length=50, style=\"informal\", focus=\"science\", keywords=\"research, experiments\")\n","\n","if summary:\n","    print(f\"Summary:\\n{summary}\")\n","\n","```\n","\n","This Python code demonstrates how to structure your prompt to include all the important elements discussed earlier:  length, style, focus, and keywords.  The core of prompt engineering is in *how you construct the `prompt` string*.  The code just provides a way to send that prompt to an LLM API. Remember to install the OpenAI library (`pip install openai`) if you want to run this code with the OpenAI API.  Adapt this structure to other LLM APIs as needed.  The key is the well-crafted prompt itself, not the specific code used to send it.\n","\n"]}],"source":["print(ask(\"Can you show me a code example?\"))"]},{"cell_type":"markdown","metadata":{"id":"Wcr1-bFHWsfW"},"source":["# Reading\n","\n","https://www.datacamp.com/blog/dspy-introduction"]},{"cell_type":"markdown","metadata":{"id":"NAMJX-aOTgcY"},"source":["#For image prompts check out DALL-E and other image generation tools.\n","*Provide your prompts and AI generated image below.*"]}],"metadata":{"colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}